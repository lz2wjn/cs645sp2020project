{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase1 Using the offline workload A as the traning data\n",
    "\n",
    "#Step1 Load data\n",
    "\n",
    "workload_A=pd.read_csv('../../offline_workload.csv') \n",
    "#print(workload_A.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2 Split workload_A by creating different dataframes\n",
    "\n",
    "workload_A_ids = workload_A[\"workload id\"].unique()\n",
    "workload_A_list = []\n",
    "for num,workload_A_id in enumerate(workload_A_ids,start=1):\n",
    "    tmp_workload = workload_A[workload_A['workload id'].isin([workload_A_id])]\n",
    "    workload_A_list.append(tmp_workload)\n",
    "    exec(\"workload_A_%s = tmp_workload\"%num)\n",
    "\n",
    "#workload_A_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3 Get metrics\n",
    "\n",
    "tmpframe = workload_A.T.copy()\n",
    "tmpframe = tmpframe.tail(585-1-12).T\n",
    "metrics_name = tmpframe.columns\n",
    "#print(metrics_name)\n",
    "workload_A_listformetrics = []\n",
    "num_workloads_A = len(workload_A_list)\n",
    "for i in range(num_workloads_A):\n",
    "    tmp = (workload_A_list[i])[metrics_name]\n",
    "    workload_A_listformetrics.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step4 FA (apply for all workloads)\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "\n",
    "transformer = FactorAnalysis(n_components=7, random_state=0)\n",
    "workload_list_afterFA = []\n",
    "\n",
    "for i in range(num_workloads_A):\n",
    "    tmp_workload_A_listformetrics = workload_A_listformetrics[i]\n",
    "    tmp_workload_A_listformetrics_array = tmp_workload_A_listformetrics.values\n",
    "    tmp_workload_A_listformetrics_array_Trans = tmp_workload_A_listformetrics_array.T\n",
    "    tmp_transformer = FactorAnalysis(n_components=7, random_state=0)\n",
    "    tmp_workload_A_listformetrics_array_Trans_transformed = transformer.fit_transform(tmp_workload_A_listformetrics_array_Trans)\n",
    "    workload_list_afterFA.append(tmp_workload_A_listformetrics_array_Trans_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5 K-means (apply for all workloads)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "workload_list_centers_metrics_name = []\n",
    "\n",
    "for i in range(num_workloads_A):\n",
    "    tmp_workload_A_listformetrics_array_Kmeans = KMeans(n_clusters=2, random_state=0).fit(workload_list_afterFA[i])\n",
    "    tmp_cluster_centers = tmp_workload_A_listformetrics_array_Kmeans.cluster_centers_\n",
    "    tmp_centers_index = np.where((workload_list_afterFA[i]==tmp_cluster_centers[:,None]).all(-1))\n",
    "    tmp_centers_metrics_name = []\n",
    "    tmp_n_centers = len(tmp_centers_index)\n",
    "    for j in range(tmp_n_centers):\n",
    "        tmp_mname = metrics_name[tmp_centers_index[j][0]]\n",
    "        tmp_centers_metrics_name.append(tmp_mname)\n",
    "    \n",
    "    workload_list_centers_metrics_name.append(tmp_centers_metrics_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step6 Get Prutiend workload (apply for all workloads)\n",
    "\n",
    "knobs_latency_list = ['k1','k2','k3','k4','k5','k6','k7','k8','s1','s2','s3','s4']\n",
    "latency = ['latency']\n",
    "\n",
    "pruning_workload_list = []\n",
    "\n",
    "for i in range(num_workloads_A):\n",
    "    tmp_pruning_list = knobs_latency_list + latency + workload_list_centers_metrics_name[i]\n",
    "    tmp_pruning_workload_A = (workload_A_list[i])[tmp_pruning_list]\n",
    "    pruning_workload_list.append(tmp_pruning_workload_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\28215\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:469: ConvergenceWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ 0.00013058, -0.00059283]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 61, 'nit': 12, 'warnflag': 2}\n",
      "  ConvergenceWarning)\n",
      "G:\\Users\\28215\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:469: ConvergenceWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-3.84483485e-04,  7.72853886e-06]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 52, 'nit': 7, 'warnflag': 2}\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Step7 Building GPR (apply for all workloads)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "workload_A_GPRmodels_lists = []\n",
    "\n",
    "for i in range(num_workloads_A):\n",
    "    tmp_GPRmodels_lists = []\n",
    "    \n",
    "    tmp_Pruning_workload_A_X = ((workload_A_list[i])[knobs_latency_list]).values\n",
    "    tmp_Pruning_workload_A_y_o = ((workload_A_list[i])[latency]).values\n",
    "    tmp_kernel = DotProduct() + WhiteKernel()\n",
    "    tmp_workload_A_gpr_o = GaussianProcessRegressor(kernel=tmp_kernel,random_state=0).fit(tmp_Pruning_workload_A_X, tmp_Pruning_workload_A_y_o)\n",
    "    \n",
    "    tmp_GPRmodels_lists.append(tmp_workload_A_gpr_o)\n",
    "    \n",
    "    tmp_num_centers = len(workload_list_centers_metrics_name[i])\n",
    "    for j in range(tmp_num_centers):\n",
    "        tmp_Pruning_workload_A_y_m = ((workload_A_list[i])[workload_list_centers_metrics_name[i][j]]).values\n",
    "        tmp_kernel = DotProduct() + WhiteKernel()\n",
    "        tmp_workload_A_gpr_m = GaussianProcessRegressor(kernel=tmp_kernel,random_state=0).fit(tmp_Pruning_workload_A_X, tmp_Pruning_workload_A_y_m)\n",
    "        tmp_GPRmodels_lists.append(tmp_workload_A_gpr_m)\n",
    "    \n",
    "    workload_A_GPRmodels_lists.append(tmp_GPRmodels_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************************************************************************************************\n",
    "#    1\n",
    "#    We have workload_A_list contain 58 workloads\n",
    "#    workload_A_list\n",
    "#\n",
    "#    2\n",
    "#    We have workload_list_centers_metrics_name contain 58 lists, each list have the metrics kept after FA and K-means for each workload\n",
    "#    workload_list_centers_metrics_name\n",
    "#\n",
    "#    3\n",
    "#    We have pruning_workload_list contain 58 workloads after prunning\n",
    "#    pruning_workload_list\n",
    "#\n",
    "#    4\n",
    "#    We have workload_A_GPRmodels_lists contain 58 lists, each list have 1(latency) + k(number of metrics kept) GPR models\n",
    "#    workload_A_GPRmodels_lists\n",
    "#*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase2 Using the online workload B as the validation data\n",
    "\n",
    "#In workloads B, each workload has 6 data points, we use 5 of them apply workload mapping, and use the left 1 as the target workload, predict\n",
    "#latency and compare it with the actual value in workloads B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1 load workload B data set\n",
    "\n",
    "workload_B=pd.read_csv('../../online_workload_B.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2 Split workload_B by creating different dataframes\n",
    "\n",
    "workload_B_ids = workload_B[\"workload id\"].unique()\n",
    "workload_B_list = []\n",
    "for num,workload_B_id in enumerate(workload_B_ids,start=1):\n",
    "    tmp_workload = workload_B[workload_B['workload id'].isin([workload_B_id])]\n",
    "    workload_B_list.append(tmp_workload)\n",
    "    exec(\"workload_B_%s = tmp_workload\"%num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3 Split each workload into 5 data points and 1 data point as target\n",
    "\n",
    "num_workloads_B = len(workload_B_list)\n",
    "\n",
    "workload_B_mapping_data = []\n",
    "workload_B_target = []\n",
    "\n",
    "for i in range(num_workloads_B):\n",
    "    workload_B_mapping_data.append(workload_B_list[i][:-1])\n",
    "    workload_B_target.append(workload_B_list[i][5:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step4 workload mapping Test for all workload in workload B\n",
    "all_distance_list = []\n",
    "for i in range(100):\n",
    "    tmp_test_workload_B_mapping = workload_B_mapping_data[i]\n",
    "    tmp_test_workload_B_mapping_cfigs = ((tmp_test_workload_B_mapping)[knobs_latency_list]).values\n",
    "    tmp_distance_list = []\n",
    "    \n",
    "    for j in range(num_workloads_A):\n",
    "        tmp_test_workload_A = workload_A_list[j]\n",
    "        tmp_test_workload_A_metrics = workload_list_centers_metrics_name[j]\n",
    "        tmp_test_workload_A_gpr_model = workload_A_GPRmodels_lists[j]\n",
    "        tmp_test_prunning_workload_A = pruning_workload_list[j]\n",
    "    \n",
    "        tmp_test_workload_A_cfigs = ((tmp_test_workload_A)[knobs_latency_list]).values\n",
    "    \n",
    "        tmp_corr_metric_values = []\n",
    "        \n",
    "        for k in range(5):\n",
    "            tmp_data_point = tmp_test_workload_B_mapping_cfigs[k]\n",
    "            tmp_check = np.where((tmp_data_point==tmp_test_workload_A_cfigs[:,None]).all(-1))\n",
    "            \n",
    "            if len(tmp_check[0]) == 0:\n",
    "                tmp_gpr_predic = []\n",
    "                for m in range(2):\n",
    "                    tmp_gpr_predic.append(tmp_test_workload_A_gpr_model[m+1].predict([tmp_test_workload_B_mapping_cfigs[m]])[0])\n",
    "                tmp_corr_metric_values.append(tmp_gpr_predic)\n",
    "            \n",
    "            else:\n",
    "                tmp_gpr_predic = []\n",
    "                tmp_row_idx = tmp_check[0][0]\n",
    "                #print(tmp_row_idx)\n",
    "                #print('xxxxxxxxxxxxxxxxxxx')\n",
    "                tmp_test_prunning_workload_A_array = tmp_test_prunning_workload_A.values\n",
    "                tmp_gpr_predic.append(tmp_test_prunning_workload_A_array[tmp_row_idx][13])\n",
    "                tmp_gpr_predic.append(tmp_test_prunning_workload_A_array[tmp_row_idx][14])\n",
    "                tmp_corr_metric_values.append(tmp_gpr_predic)\n",
    "        \n",
    "        tmp_test_workload_B_mapping_metrics_value = tmp_test_workload_B_mapping[tmp_test_workload_A_metrics].values\n",
    "        #print(tmp_test_workload_B_mapping_metrics_value)\n",
    "        #print('--------')\n",
    "        tmp_distance = np.sum((tmp_corr_metric_values-tmp_test_workload_B_mapping_metrics_value)*(tmp_corr_metric_values-tmp_test_workload_B_mapping_metrics_value))\n",
    "        tmp_distance_list.append(tmp_distance)\n",
    "     \n",
    "    all_distance_list.append(tmp_distance_list)\n",
    "\n",
    "\n",
    "    \n",
    "all_min_idx_list = []\n",
    "for i in range(100):\n",
    "    tmp_idx = all_distance_list[i].index(min(all_distance_list[i]))\n",
    "    all_min_idx_list.append(tmp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\28215\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:469: ConvergenceWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([6.91159668e-05, 7.95012220e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 66, 'nit': 16, 'warnflag': 2}\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Step5 Concatenate trances from the observed target workload and the nearest neighbor workload for all workload B\n",
    "\n",
    "predictGPRmodel_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    tmp_similar_idx = all_min_idx_list[i]\n",
    "    tmp_similar_workload = pruning_workload_list[tmp_similar_idx]\n",
    "    tmp_similar_workload_configs = ((pruning_workload_list[tmp_similar_idx])[knobs_latency_list]).values\n",
    "    tmp_similar_workload_metrics = ((pruning_workload_list[tmp_similar_idx])[latency + workload_list_centers_metrics_name[tmp_similar_idx]]).values\n",
    "    tmp_workload_B_mapping_metrics_with_similar = ((workload_B_mapping_data[i])[latency + workload_list_centers_metrics_name[tmp_similar_idx]]).values\n",
    "    \n",
    "    tmp_Augmented_pruned_target_configs = np.array(tmp_similar_workload_configs)\n",
    "    tmp_Augmented_pruned_target_metrics = np.array(tmp_similar_workload_metrics)\n",
    "    \n",
    "    tmp_workload_B_configs = workload_B_mapping_data[i][knobs_latency_list].values\n",
    "    #print(tmp_workload_B_configs)\n",
    "    \n",
    "    for j in range(5):\n",
    "        tmp_test_workload_B_mapping_cfigs = tmp_workload_B_configs[j]\n",
    "        tmp_test_workload_B_mapping_metrics_with_similar = tmp_workload_B_mapping_metrics_with_similar[j]\n",
    "        tmp_check = np.where((tmp_test_workload_B_mapping_cfigs==tmp_similar_workload_configs[:,None]).all(-1))\n",
    "        \n",
    "        if len(tmp_check[0]) == 0:\n",
    "            tmp_Augmented_pruned_target_configs = np.insert(tmp_Augmented_pruned_target_configs,0,tmp_test_workload_B_mapping_cfigs,axis=0)\n",
    "            tmp_Augmented_pruned_target_metrics = np.insert(tmp_Augmented_pruned_target_metrics,0,tmp_test_workload_B_mapping_metrics_with_similar,axis=0)\n",
    "        else:\n",
    "        \n",
    "            tmp_row_idx = tmp_check[0][0]\n",
    "            #print(tmp_row_idx)\n",
    "            #print('xxxxxxxxxxxxxxxxxxx')\n",
    "            tmp_Augmented_pruned_target_metrics[tmp_row_idx] = tmp_test_workload_B_mapping_metrics_with_similar\n",
    "    \n",
    "    test_workload_B_gpr_o = GaussianProcessRegressor(kernel=tmp_kernel,random_state=0).fit(tmp_Augmented_pruned_target_configs, tmp_Augmented_pruned_target_metrics.T[0])\n",
    "    predictGPRmodel_list.append(test_workload_B_gpr_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.01275399]\n",
      "   latency\n",
      "5    7.923\n",
      "------------\n",
      "[468.89355157]\n",
      "      latency\n",
      "11  260.62625\n",
      "------------\n",
      "[140.57925878]\n",
      "     latency\n",
      "17  109.0855\n",
      "------------\n",
      "[423.68501883]\n",
      "    latency\n",
      "23   61.997\n",
      "------------\n",
      "[190.90305545]\n",
      "     latency\n",
      "29  89.78475\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "#Step6 predict for all workload B\n",
    "\n",
    "for i in range(5):\n",
    "    tmp_target_config = workload_B_target[i][knobs_latency_list].values\n",
    "    tmp_predict = predictGPRmodel_list[i].predict(tmp_target_config)\n",
    "    print(tmp_predict)\n",
    "    print(workload_B_target[i][latency])\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
