{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.4940028190612793 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#Phase1 Using the offline workload A as the traning data\n",
    "\n",
    "#Step1 Load data\n",
    "\n",
    "workload_A=pd.read_csv('../../offline_workload.csv') \n",
    "#print(workload_A.head())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.10571646690368652 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step2 Split workload_A by creating different dataframes\n",
    "\n",
    "workload_A_ids = workload_A[\"workload id\"].unique()\n",
    "workload_A_list = []\n",
    "for num,workload_A_id in enumerate(workload_A_ids,start=1):\n",
    "    tmp_workload = workload_A[workload_A['workload id'].isin([workload_A_id])]\n",
    "    workload_A_list.append(tmp_workload)\n",
    "    exec(\"workload_A_%s = tmp_workload\"%num)\n",
    "\n",
    "#workload_A_1\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.3197710514068604 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step3 Get metrics\n",
    "\n",
    "tmpframe = workload_A.T.copy()\n",
    "tmpframe = tmpframe.tail(585-1-12).T\n",
    "metrics_name = tmpframe.columns\n",
    "#print(metrics_name)\n",
    "workload_A_listformetrics = []\n",
    "num_workloads_A = len(workload_A_list)\n",
    "for i in range(num_workloads_A):\n",
    "    tmp = (workload_A_list[i])[metrics_name]\n",
    "    workload_A_listformetrics.append(tmp)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572, 18349)\n",
      "(572, 300)\n",
      "--- 15.695021152496338 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Step4 FA for all workloads together\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "all_metrics_data = tmpframe.values\n",
    "\n",
    "all_metrics_data_normalized = normalize(all_metrics_data, norm='max')\n",
    "all_metrics_data_normalized_Trans = all_metrics_data_normalized.T\n",
    "\n",
    "#all_metrics_data_Trans = all_metrics_data.T\n",
    "#all_metrics_data_normalized_Trans = normalize(all_metrics_data_Trans, norm='max')\n",
    "\n",
    "print(all_metrics_data_Trans.shape)\n",
    "tmp_all_transformer = FactorAnalysis(n_components=300, random_state=0)\n",
    "tmp_workload_A_transformed = tmp_all_transformer.fit_transform(all_metrics_data_normalized_Trans)\n",
    "print(tmp_workload_A_transformed.shape)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 90, 126]\n",
      "['executor.jvmGCTime.avg', 'driver.jvm.pools.PS-Eden-Space.committed.avg', 'executor.cpuTime.avg']\n"
     ]
    }
   ],
   "source": [
    "#Step5 K-means for all workloads together\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_M = 3\n",
    "\n",
    "workload_A_list_array_Kmeans = KMeans(n_clusters=num_M, random_state=0).fit(tmp_workload_A_transformed)\n",
    "cluster_centers = workload_A_list_array_Kmeans.cluster_centers_\n",
    "tmp_centers_index = []\n",
    "tmp_centers_metrics_name = []\n",
    "for i in range(num_M):\n",
    "    tmp_center = cluster_centers[i]\n",
    "    tmp_center_diff = tmp_center - tmp_workload_A_transformed\n",
    "    tmp_center_distance = np.sum((tmp_center_diff * tmp_center_diff),axis=1)\n",
    "    tmp_center_min_idx = np.argmin(tmp_center_distance)\n",
    "    tmp_centers_index.append(tmp_center_min_idx)\n",
    "    \n",
    "for j in range(num_M):\n",
    "        tmp_mname = metrics_name[tmp_centers_index[j]]\n",
    "        #print(tmp_mname)\n",
    "        tmp_centers_metrics_name.append(tmp_mname)\n",
    "print(tmp_centers_index)\n",
    "print(tmp_centers_metrics_name)\n",
    "\n",
    "workload_list_centers_metrics_name = []\n",
    "for n in range(58):\n",
    "    workload_list_centers_metrics_name.append(tmp_centers_metrics_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>executor.jvmGCTime.avg</th>\n",
       "      <th>driver.jvm.pools.PS-Eden-Space.committed.avg</th>\n",
       "      <th>executor.cpuTime.avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2871.972673</td>\n",
       "      <td>1.657668e+09</td>\n",
       "      <td>1.826837e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5305.223214</td>\n",
       "      <td>1.725497e+09</td>\n",
       "      <td>6.162674e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3218.352083</td>\n",
       "      <td>1.583612e+09</td>\n",
       "      <td>6.299830e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2586.837054</td>\n",
       "      <td>1.577189e+09</td>\n",
       "      <td>5.983338e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2244.046875</td>\n",
       "      <td>1.357316e+09</td>\n",
       "      <td>1.563849e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18344</th>\n",
       "      <td>13871.422595</td>\n",
       "      <td>1.731723e+09</td>\n",
       "      <td>3.884193e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18345</th>\n",
       "      <td>16487.433314</td>\n",
       "      <td>1.703937e+09</td>\n",
       "      <td>4.016819e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18346</th>\n",
       "      <td>13037.349878</td>\n",
       "      <td>1.186276e+09</td>\n",
       "      <td>6.326932e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18347</th>\n",
       "      <td>34443.626830</td>\n",
       "      <td>1.768948e+09</td>\n",
       "      <td>1.757525e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18348</th>\n",
       "      <td>40417.752708</td>\n",
       "      <td>1.803791e+09</td>\n",
       "      <td>2.191186e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18349 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       executor.jvmGCTime.avg  driver.jvm.pools.PS-Eden-Space.committed.avg  \\\n",
       "0                 2871.972673                                  1.657668e+09   \n",
       "1                 5305.223214                                  1.725497e+09   \n",
       "2                 3218.352083                                  1.583612e+09   \n",
       "3                 2586.837054                                  1.577189e+09   \n",
       "4                 2244.046875                                  1.357316e+09   \n",
       "...                       ...                                           ...   \n",
       "18344            13871.422595                                  1.731723e+09   \n",
       "18345            16487.433314                                  1.703937e+09   \n",
       "18346            13037.349878                                  1.186276e+09   \n",
       "18347            34443.626830                                  1.768948e+09   \n",
       "18348            40417.752708                                  1.803791e+09   \n",
       "\n",
       "       executor.cpuTime.avg  \n",
       "0              1.826837e+11  \n",
       "1              6.162674e+10  \n",
       "2              6.299830e+10  \n",
       "3              5.983338e+10  \n",
       "4              1.563849e+11  \n",
       "...                     ...  \n",
       "18344          3.884193e+11  \n",
       "18345          4.016819e+11  \n",
       "18346          6.326932e+11  \n",
       "18347          1.757525e+12  \n",
       "18348          2.191186e+12  \n",
       "\n",
       "[18349 rows x 3 columns]"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_view_metric = workload_A[tmp_centers_metrics_name]\n",
    "quick_view_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_time = time.time()\\n\\n#Step4 FA (apply for all workloads)\\nfrom sklearn.decomposition import FactorAnalysis\\nfrom sklearn.preprocessing import normalize\\n\\n#transformer = FactorAnalysis(n_components=7, random_state=0)\\nworkload_list_afterFA = []\\n\\nfor i in range(58):\\n    tmp_workload_A_listformetrics = workload_A_listformetrics[i]\\n    tmp_workload_A_listformetrics_array = tmp_workload_A_listformetrics.values\\n    #print(tmp_workload_A_listformetrics)\\n    tmp_workload_A_listformetrics_array_normalized = normalize(tmp_workload_A_listformetrics_array, norm=\\'l2\\')\\n    tmp_workload_A_listformetrics_array_normalized_Trans = tmp_workload_A_listformetrics_array_normalized.T\\n    \\n    #tmp_workload_A_listformetrics_array_Trans = tmp_workload_A_listformetrics_array.T\\n    #tmp_workload_A_listformetrics_array_Trans = tmp_workload_A_listformetrics_array.T\\n    #tmp_workload_A_listformetrics_array_normalized = normalize(tmp_workload_A_listformetrics_array_Trans, norm=\\'l2\\')\\n    tmp_transformer = FactorAnalysis(n_components=20, random_state=0)\\n    #print(tmp_workload_A_listformetrics_array_Trans)\\n    #normalize tmp_workload_A_listformetrics_array_Trans\\n    #tmp_workload_A_listformetrics_array_Trans_normalized = normalize(tmp_workload_A_listformetrics_array_Trans, norm=\\'l2\\')\\n    #print(\\'-------------------------\\')\\n    #print(tmp_workload_A_listformetrics_array_Trans_normalized)\\n    \\n    tmp_workload_A_listformetrics_array_Trans_transformed = tmp_transformer.fit_transform(tmp_workload_A_listformetrics_array_normalized_Trans)\\n    workload_list_afterFA.append(tmp_workload_A_listformetrics_array_Trans_transformed)\\n\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\n#print(workload_list_afterFA[0].shape)\\n'"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''start_time = time.time()\n",
    "\n",
    "#Step4 FA (apply for all workloads)\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#transformer = FactorAnalysis(n_components=7, random_state=0)\n",
    "workload_list_afterFA = []\n",
    "\n",
    "for i in range(58):\n",
    "    tmp_workload_A_listformetrics = workload_A_listformetrics[i]\n",
    "    tmp_workload_A_listformetrics_array = tmp_workload_A_listformetrics.values\n",
    "    #print(tmp_workload_A_listformetrics)\n",
    "    tmp_workload_A_listformetrics_array_normalized = normalize(tmp_workload_A_listformetrics_array, norm='l2')\n",
    "    tmp_workload_A_listformetrics_array_normalized_Trans = tmp_workload_A_listformetrics_array_normalized.T\n",
    "    \n",
    "    #tmp_workload_A_listformetrics_array_Trans = tmp_workload_A_listformetrics_array.T\n",
    "    #tmp_workload_A_listformetrics_array_Trans = tmp_workload_A_listformetrics_array.T\n",
    "    #tmp_workload_A_listformetrics_array_normalized = normalize(tmp_workload_A_listformetrics_array_Trans, norm='l2')\n",
    "    tmp_transformer = FactorAnalysis(n_components=20, random_state=0)\n",
    "    #print(tmp_workload_A_listformetrics_array_Trans)\n",
    "    #normalize tmp_workload_A_listformetrics_array_Trans\n",
    "    #tmp_workload_A_listformetrics_array_Trans_normalized = normalize(tmp_workload_A_listformetrics_array_Trans, norm='l2')\n",
    "    #print('-------------------------')\n",
    "    #print(tmp_workload_A_listformetrics_array_Trans_normalized)\n",
    "    \n",
    "    tmp_workload_A_listformetrics_array_Trans_transformed = tmp_transformer.fit_transform(tmp_workload_A_listformetrics_array_normalized_Trans)\n",
    "    workload_list_afterFA.append(tmp_workload_A_listformetrics_array_Trans_transformed)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#print(workload_list_afterFA[0].shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_time = time.time()\\n\\n#Step5 K-means (apply for all workloads)\\nfrom sklearn.cluster import KMeans\\n\\nnum_M = 2\\n\\nworkload_list_centers_metrics_name = []\\n\\nfor i in range(58):\\n    tmp_workload_A_listformetrics_array_Kmeans = KMeans(n_clusters=num_M, random_state=0).fit(workload_list_afterFA[i])\\n    tmp_cluster_centers = tmp_workload_A_listformetrics_array_Kmeans.cluster_centers_\\n    #print(tmp_cluster_centers)\\n    tmp_centers_index = []\\n    tmp_centers_metrics_name= []\\n    #print(workload_list_afterFA[i].shape)\\n    for j in range(num_M):\\n        tmp_center = tmp_cluster_centers[j]\\n        tmp_center_diff = tmp_center - workload_list_afterFA[i]\\n        tmp_center_distance = np.sum((tmp_center_diff * tmp_center_diff),axis=1)\\n        tmp_center_min_idx = np.argmin(tmp_center_distance)\\n        tmp_centers_index.append(tmp_center_min_idx)\\n        #print(tmp_center_min_idx)\\n        #print(\\'-----\\')\\n    #tmp_centers_metrics_name = []\\n    tmp_n_centers = len(tmp_cluster_centers)\\n    #print(tmp_n_centers)\\n    for j in range(tmp_n_centers):\\n        tmp_mname = metrics_name[tmp_centers_index[j]]\\n        #print(tmp_mname)\\n        tmp_centers_metrics_name.append(tmp_mname)\\n    \\n    workload_list_centers_metrics_name.append(tmp_centers_metrics_name)\\n\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\nprint(workload_list_centers_metrics_name[0])\\n'"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''start_time = time.time()\n",
    "\n",
    "#Step5 K-means (apply for all workloads)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_M = 2\n",
    "\n",
    "workload_list_centers_metrics_name = []\n",
    "\n",
    "for i in range(58):\n",
    "    tmp_workload_A_listformetrics_array_Kmeans = KMeans(n_clusters=num_M, random_state=0).fit(workload_list_afterFA[i])\n",
    "    tmp_cluster_centers = tmp_workload_A_listformetrics_array_Kmeans.cluster_centers_\n",
    "    #print(tmp_cluster_centers)\n",
    "    tmp_centers_index = []\n",
    "    tmp_centers_metrics_name= []\n",
    "    #print(workload_list_afterFA[i].shape)\n",
    "    for j in range(num_M):\n",
    "        tmp_center = tmp_cluster_centers[j]\n",
    "        tmp_center_diff = tmp_center - workload_list_afterFA[i]\n",
    "        tmp_center_distance = np.sum((tmp_center_diff * tmp_center_diff),axis=1)\n",
    "        tmp_center_min_idx = np.argmin(tmp_center_distance)\n",
    "        tmp_centers_index.append(tmp_center_min_idx)\n",
    "        #print(tmp_center_min_idx)\n",
    "        #print('-----')\n",
    "    #tmp_centers_metrics_name = []\n",
    "    tmp_n_centers = len(tmp_cluster_centers)\n",
    "    #print(tmp_n_centers)\n",
    "    for j in range(tmp_n_centers):\n",
    "        tmp_mname = metrics_name[tmp_centers_index[j]]\n",
    "        #print(tmp_mname)\n",
    "        tmp_centers_metrics_name.append(tmp_mname)\n",
    "    \n",
    "    workload_list_centers_metrics_name.append(tmp_centers_metrics_name)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(workload_list_centers_metrics_name[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.043883562088012695 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step6 Get Prutiend workload (apply for all workloads)\n",
    "\n",
    "knobs_latency_list = ['k1','k2','k3','k4','k5','k6','k7','k8','s1','s2','s3','s4']\n",
    "latency = ['latency']\n",
    "pruning_workload_list = []\n",
    "\n",
    "for i in range(num_workloads_A):\n",
    "    #tmp_pruning_list = knobs_latency_list + latency + tmp_centers_metrics_name\n",
    "    tmp_pruning_list = knobs_latency_list + latency + workload_list_centers_metrics_name[i]\n",
    "    tmp_pruning_workload_A = (workload_A_list[i])[tmp_pruning_list]\n",
    "    pruning_workload_list.append(tmp_pruning_workload_A)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 36.881141662597656 seconds ---\n",
      "[MinMaxScaler(copy=True, feature_range=(0, 1)), MinMaxScaler(copy=True, feature_range=(0, 1)), MinMaxScaler(copy=True, feature_range=(0, 1)), MinMaxScaler(copy=True, feature_range=(0, 1)), MinMaxScaler(copy=True, feature_range=(0, 1))]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step7 Building GPR (apply for all workloads)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "tmp_kernel = 1.0 * RBF(length_scale=1.0)\n",
    "\n",
    "workload_A_GPRmodels_lists = []\n",
    "workload_A_scalar_lists = []\n",
    "workload_A_normalized_m = []\n",
    "\n",
    "for i in range(58):\n",
    "    tmp_GPRmodels_lists = []\n",
    "    tmp_scalar_lists = []\n",
    "    tmp_normalized_m = []\n",
    "    \n",
    "    tmp_Pruning_workload_A_X = ((workload_A_list[i])[knobs_latency_list]).values\n",
    "    tmp_Pruning_workload_A_y_o = ((workload_A_list[i])[latency]).values\n",
    "    #print(tmp_Pruning_workload_A_y_o)\n",
    "    \n",
    "    tmp_x_scaler = MinMaxScaler().fit(tmp_Pruning_workload_A_X)\n",
    "    tmp_y_o_scaler = MinMaxScaler().fit(tmp_Pruning_workload_A_y_o)\n",
    "    \n",
    "    tmp_scalar_lists.append(tmp_x_scaler)\n",
    "    tmp_scalar_lists.append(tmp_y_o_scaler)\n",
    "    \n",
    "    tmp_Pruning_workload_A_X_normalized = tmp_x_scaler.transform(tmp_Pruning_workload_A_X)\n",
    "    #print(tmp_x_scaler.data_min_)\n",
    "    tmp_Pruning_workload_A_y_o_normalized = tmp_y_o_scaler.transform(tmp_Pruning_workload_A_y_o)\n",
    "    \n",
    "    tmp_normalized_m.append(tmp_Pruning_workload_A_y_o_normalized)\n",
    "    #print(tmp_Pruning_workload_A_y_o_normalized)\n",
    "    \n",
    "    #tmp_kernel = DotProduct() + WhiteKernel()\n",
    "    tmp_workload_A_gpr_o = GaussianProcessRegressor(kernel=tmp_kernel).fit(tmp_Pruning_workload_A_X_normalized, tmp_Pruning_workload_A_y_o_normalized)\n",
    "    \n",
    "    tmp_GPRmodels_lists.append(tmp_workload_A_gpr_o)\n",
    "    \n",
    "    tmp_num_centers = len(workload_list_centers_metrics_name[i])\n",
    "    for j in range(tmp_num_centers):\n",
    "        \n",
    "        tmp_Pruning_workload_A_y_m = ((workload_A_list[i])[[workload_list_centers_metrics_name[i][j]]]).values\n",
    "        #print(tmp_Pruning_workload_A_y_m)\n",
    "        tmp_y_m_scaler = MinMaxScaler().fit(tmp_Pruning_workload_A_y_m)\n",
    "        tmp_scalar_lists.append(tmp_y_m_scaler)\n",
    "        tmp_Pruning_workload_A_y_m_normalized = tmp_y_m_scaler.transform(tmp_Pruning_workload_A_y_m)\n",
    "        tmp_normalized_m.append(tmp_Pruning_workload_A_y_m_normalized)\n",
    "        \n",
    "        #tmp_kernel = DotProduct() + WhiteKernel()\n",
    "        tmp_workload_A_gpr_m = GaussianProcessRegressor(kernel=tmp_kernel).fit(tmp_Pruning_workload_A_X_normalized, tmp_Pruning_workload_A_y_m_normalized)\n",
    "        tmp_GPRmodels_lists.append(tmp_workload_A_gpr_m)\n",
    "        \n",
    "    workload_A_GPRmodels_lists.append(tmp_GPRmodels_lists)\n",
    "    workload_A_scalar_lists.append(tmp_scalar_lists)\n",
    "    workload_A_normalized_m.append(tmp_normalized_m)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#print(workload_A_normalized_m)\n",
    "print(workload_A_scalar_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************************************************************************************************\n",
    "#    1\n",
    "#    We have workload_A_list contain 58 workloads\n",
    "#    workload_A_list\n",
    "#\n",
    "#    2\n",
    "#    We have workload_list_centers_metrics_name contain 58 lists, each list have the metrics kept after FA and K-means for each workload\n",
    "#    workload_list_centers_metrics_name\n",
    "#\n",
    "#    3\n",
    "#    We have pruning_workload_list contain 58 workloads after prunning\n",
    "#    pruning_workload_list\n",
    "#\n",
    "#    4\n",
    "#    We have workload_A_GPRmodels_lists contain 58 lists, each list have 1(latency) + k(number of metrics kept) GPR models\n",
    "#    workload_A_GPRmodels_lists\n",
    "#\n",
    "#*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['executor.jvmGCTime.avg',\n",
       " 'driver.jvm.pools.PS-Eden-Space.committed.avg',\n",
       " 'executor.cpuTime.avg']"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workload_list_centers_metrics_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase2 Using the online workload B as the validation data\n",
    "\n",
    "#In workloads B, each workload has 6 data points, we use 5 of them apply workload mapping, and use the left 1 as the target workload, predict\n",
    "#latency and compare it with the actual value in workloads B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.08078289031982422 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step1 load workload B data set\n",
    "\n",
    "workload_B=pd.read_csv('../../online_workload_B.csv') \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.06981492042541504 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step2 Split workload_B by creating different dataframes\n",
    "\n",
    "workload_B_ids = workload_B[\"workload id\"].unique()\n",
    "workload_B_list = []\n",
    "for num,workload_B_id in enumerate(workload_B_ids,start=1):\n",
    "    tmp_workload = workload_B[workload_B['workload id'].isin([workload_B_id])]\n",
    "    workload_B_list.append(tmp_workload)\n",
    "    exec(\"workload_B_%s = tmp_workload\"%num)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.03992176055908203 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step3 Split each workload into 5 data points and 1 data point as target\n",
    "\n",
    "num_workloads_B = len(workload_B_list)\n",
    "\n",
    "workload_B_mapping_data = []\n",
    "workload_B_target = []\n",
    "\n",
    "for i in range(num_workloads_B):\n",
    "    workload_B_mapping_data.append(workload_B_list[i][:-1])\n",
    "    workload_B_target.append(workload_B_list[i][5:6])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 23.87873125076294 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step4 workload mapping Test for all workload in workload B\n",
    "all_distance_list = []\n",
    "for i in range(100):\n",
    "    tmp_test_workload_B_mapping = workload_B_mapping_data[i]\n",
    "    tmp_test_workload_B_mapping_cfigs = ((tmp_test_workload_B_mapping)[knobs_latency_list]).values\n",
    "    tmp_distance_list = []\n",
    "    \n",
    "    for j in range(58):\n",
    "        tmp_test_workload_A = workload_A_list[j]\n",
    "        tmp_test_workload_A_metrics = workload_list_centers_metrics_name[j]\n",
    "        tmp_test_workload_A_gpr_model = workload_A_GPRmodels_lists[j]\n",
    "        tmp_test_prunning_workload_A = pruning_workload_list[j]\n",
    "        tmp_test_workload_A_cigs_scalar = workload_A_scalar_lists[j][0]\n",
    "        tmp_test_workload_A_normalized_m = workload_A_normalized_m[j]\n",
    "        \n",
    "        tmp_norm_workload_B_mapping_cfigs = tmp_test_workload_A_cigs_scalar.transform(tmp_test_workload_B_mapping_cfigs)\n",
    "        #print(tmp_norm_workload_B_mapping_cfigs)\n",
    "        \n",
    "        tmp_test_workload_A_cfigs = ((tmp_test_workload_A)[knobs_latency_list]).values\n",
    "        #print(tmp_test_workload_A_gpr_model)\n",
    "        tmp_corr_metric_values = []\n",
    "        \n",
    "        for k in range(5):\n",
    "            tmp_data_point = tmp_test_workload_B_mapping_cfigs[k]\n",
    "            tmp_check = np.where((tmp_data_point==tmp_test_workload_A_cfigs[:,None]).all(-1))\n",
    "            \n",
    "            if len(tmp_check[0]) == 0:\n",
    "                tmp_gpr_predic = []\n",
    "                for v in range(num_M):\n",
    "                    tmp_gpr_predic.append(tmp_test_workload_A_gpr_model[v+1].predict([tmp_norm_workload_B_mapping_cfigs[k]])[0])\n",
    "                tmp_corr_metric_values.append(tmp_gpr_predic)\n",
    "                #print(tmp_gpr_predic)\n",
    "            \n",
    "            else:\n",
    "                tmp_gpr_predic = []\n",
    "                tmp_row_idx = tmp_check[0][0]\n",
    "                #print(tmp_row_idx)\n",
    "                #print('xxxxxxxxxxxxxxxxxxx')\n",
    "                tmp_test_prunning_workload_A_array = tmp_test_prunning_workload_A.values\n",
    "                #print(tmp_test_workload_A_normalized_m)\n",
    "                for n in range(num_M):\n",
    "                    #tmp_gpr_predic.append(tmp_test_prunning_workload_A_array[tmp_row_idx][13+n])\n",
    "                    tmp_gpr_predic.append(tmp_test_workload_A_normalized_m[n+1][tmp_row_idx])\n",
    "                    \n",
    "                #tmp_gpr_predic.append(tmp_test_prunning_workload_A_array[tmp_row_idx][13])\n",
    "                #tmp_gpr_predic.append(tmp_test_prunning_workload_A_array[tmp_row_idx][14])\n",
    "                tmp_corr_metric_values.append(tmp_gpr_predic)\n",
    "        \n",
    "        tmp_test_workload_B_mapping_metrics_norm_value_list = []\n",
    "        for m in range(num_M):\n",
    "            tmp_test_workload_B_mapping_metrics_value = tmp_test_workload_B_mapping[[tmp_test_workload_A_metrics[m]]].values\n",
    "            tmp_test_workload_B_mapping_metrics_nomr_value = workload_A_scalar_lists[j][2+m].transform(tmp_test_workload_B_mapping_metrics_value)\n",
    "            tmp_test_workload_B_mapping_metrics_norm_value_list.append(tmp_test_workload_B_mapping_metrics_nomr_value)\n",
    "            \n",
    "            \n",
    "        #tmp_test_workload_B_mapping_metrics_norm_value_arry = np.asarray(tmp_test_workload_B_mapping_metrics_norm_value_list).reshape((2,5))\n",
    "        #tmp_corr_metric_values_arry = np.asarray(tmp_corr_metric_values).reshape((2,5))\n",
    "        #print(tmp_test_workload_B_mapping_metrics_norm_value_list)\n",
    "        \n",
    "        #print('---')\n",
    "        #print(tmp_corr_metric_values)\n",
    "        #print('xxxxxxxxxxxxxx')\n",
    "        tmp_test_workload_B_mapping_metrics_norm_value_arry = np.asarray(tmp_test_workload_B_mapping_metrics_norm_value_list).reshape((num_M,5))       \n",
    "        tmp_corr_metric_values_arry = np.asarray(tmp_corr_metric_values).reshape((5,num_M)).T\n",
    "        #print(tmp_test_workload_B_mapping_metrics_norm_value_arry)\n",
    "        #print('-----')\n",
    "        #print(tmp_corr_metric_values_arry)\n",
    "        \n",
    "        \n",
    "        #tmp_test_workload_B_mapping_metrics_value = tmp_test_workload_B_mapping[tmp_test_workload_A_metrics].values\n",
    "        #print(tmp_test_workload_B_mapping_metrics_value)\n",
    "        #print('--------')\n",
    "        tmp_distance = np.sum((tmp_test_workload_B_mapping_metrics_norm_value_arry-tmp_corr_metric_values_arry)*(tmp_test_workload_B_mapping_metrics_norm_value_arry-tmp_corr_metric_values_arry))\n",
    "        tmp_distance_list.append(tmp_distance)\n",
    "     \n",
    "    all_distance_list.append(tmp_distance_list)\n",
    "\n",
    "\n",
    "    \n",
    "all_min_idx_list = []\n",
    "for i in range(100):\n",
    "    tmp_idx = all_distance_list[i].index(min(all_distance_list[i]))\n",
    "    all_min_idx_list.append(tmp_idx)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 16.334603548049927 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step5 Concatenate trances from the observed target workload and the nearest neighbor workload for all workload B\n",
    "\n",
    "predictGPRmodel_list = []\n",
    "predictScalar_list = []\n",
    "inputScalar_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    tmp_similar_idx = all_min_idx_list[i]\n",
    "    tmp_similar_workload = pruning_workload_list[tmp_similar_idx]\n",
    "    tmp_similar_workload_configs = ((pruning_workload_list[tmp_similar_idx])[knobs_latency_list]).values\n",
    "    tmp_similar_workload_metrics = ((pruning_workload_list[tmp_similar_idx])[latency + workload_list_centers_metrics_name[tmp_similar_idx]]).values\n",
    "    tmp_workload_B_mapping_metrics_with_similar = ((workload_B_mapping_data[i])[latency + workload_list_centers_metrics_name[tmp_similar_idx]]).values\n",
    "    \n",
    "    tmp_Augmented_pruned_target_configs = np.array(tmp_similar_workload_configs)\n",
    "    tmp_Augmented_pruned_target_metrics = np.array(tmp_similar_workload_metrics)\n",
    "    \n",
    "    tmp_workload_B_configs = workload_B_mapping_data[i][knobs_latency_list].values\n",
    "    #print(tmp_workload_B_configs)\n",
    "    \n",
    "    for j in range(5):\n",
    "        tmp_test_workload_B_mapping_cfigs = tmp_workload_B_configs[j]\n",
    "        tmp_test_workload_B_mapping_metrics_with_similar = tmp_workload_B_mapping_metrics_with_similar[j]\n",
    "        tmp_check = np.where((tmp_test_workload_B_mapping_cfigs==tmp_similar_workload_configs[:,None]).all(-1))\n",
    "        \n",
    "        if len(tmp_check[0]) == 0:\n",
    "            #print('z')\n",
    "            tmp_Augmented_pruned_target_configs = np.insert(tmp_Augmented_pruned_target_configs,0,tmp_test_workload_B_mapping_cfigs,axis=0)\n",
    "            tmp_Augmented_pruned_target_metrics = np.insert(tmp_Augmented_pruned_target_metrics,0,tmp_test_workload_B_mapping_metrics_with_similar,axis=0)\n",
    "        else:\n",
    "        \n",
    "            tmp_row_idx = tmp_check[0][0]\n",
    "            #print(tmp_row_idx)\n",
    "            #print('xxxxxxxxxxxxxxxxxxx')\n",
    "            tmp_Augmented_pruned_target_metrics[tmp_row_idx] = tmp_test_workload_B_mapping_metrics_with_similar\n",
    "    \n",
    "    tmp_metrics_o_size = tmp_Augmented_pruned_target_metrics.T[0].shape[0]\n",
    "    tmp_nor_tmp_Augmented_pruned_target_metrics_o = tmp_Augmented_pruned_target_metrics.T[0].reshape((tmp_metrics_o_size,1))\n",
    "    tmp_o_scalar = MinMaxScaler().fit(tmp_nor_tmp_Augmented_pruned_target_metrics_o)\n",
    "    predictScalar_list.append(tmp_o_scalar)\n",
    "    \n",
    "    tmp_Augmented_pruned_target_metrics_nomr = tmp_o_scalar.transform(tmp_nor_tmp_Augmented_pruned_target_metrics_o)\n",
    "    \n",
    "    tmp_X_scalar = MinMaxScaler().fit(tmp_Augmented_pruned_target_configs)\n",
    "    inputScalar_list.append(tmp_X_scalar)\n",
    "    \n",
    "    tmp_Augmented_pruned_target_configs_norm = tmp_X_scalar.transform(tmp_Augmented_pruned_target_configs)\n",
    "    #print(tmp_Augmented_pruned_target_metrics_nomr)\n",
    "    \n",
    "    #print(tmp_Augmented_pruned_target_configs)\n",
    "    #print('------')\n",
    "    #print(tmp_nor_tmp_Augmented_pruned_target_metrics_o)\n",
    "    \n",
    "    test_workload_B_gpr_o = GaussianProcessRegressor(kernel=tmp_kernel).fit(tmp_Augmented_pruned_target_configs_norm, tmp_Augmented_pruned_target_metrics_nomr)\n",
    "    predictGPRmodel_list.append(test_workload_B_gpr_o)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict:  [[2.189625]]\n",
      "Ground Truth:  [[7.923]]\n",
      "diff:  [[5.733375]]\n",
      "------------\n",
      "Predict:  [[261.46899986]]\n",
      "Ground Truth:  [[260.62625]]\n",
      "diff:  [[0.84274986]]\n",
      "------------\n",
      "Predict:  [[16.43549996]]\n",
      "Ground Truth:  [[109.0855]]\n",
      "diff:  [[92.65000004]]\n",
      "------------\n",
      "Predict:  [[36.735]]\n",
      "Ground Truth:  [[61.997]]\n",
      "diff:  [[25.262]]\n",
      "------------\n",
      "Predict:  [[90.59599992]]\n",
      "Ground Truth:  [[89.78475]]\n",
      "diff:  [[0.81124992]]\n",
      "------------\n",
      "Predict:  [[4.39975]]\n",
      "Ground Truth:  [[24.4785]]\n",
      "diff:  [[20.07875]]\n",
      "------------\n",
      "Predict:  [[5.87975]]\n",
      "Ground Truth:  [[4.507125]]\n",
      "diff:  [[1.372625]]\n",
      "------------\n",
      "Predict:  [[18.2167499]]\n",
      "Ground Truth:  [[107.993]]\n",
      "diff:  [[89.7762501]]\n",
      "------------\n",
      "Predict:  [[285.28199981]]\n",
      "Ground Truth:  [[42.31425]]\n",
      "diff:  [[242.96774981]]\n",
      "------------\n",
      "Predict:  [[56.61299999]]\n",
      "Ground Truth:  [[52.674]]\n",
      "diff:  [[3.93899999]]\n",
      "------------\n",
      "Predict:  [[2.168]]\n",
      "Ground Truth:  [[3.084]]\n",
      "diff:  [[0.916]]\n",
      "------------\n",
      "Predict:  [[9.4355]]\n",
      "Ground Truth:  [[9.70575]]\n",
      "diff:  [[0.27025]]\n",
      "------------\n",
      "Predict:  [[224.81274976]]\n",
      "Ground Truth:  [[46.812]]\n",
      "diff:  [[178.00074976]]\n",
      "------------\n",
      "Predict:  [[2.60175]]\n",
      "Ground Truth:  [[24.076]]\n",
      "diff:  [[21.47425]]\n",
      "------------\n",
      "Predict:  [[4.7155]]\n",
      "Ground Truth:  [[8.29225]]\n",
      "diff:  [[3.57675]]\n",
      "------------\n",
      "Predict:  [[29.50599999]]\n",
      "Ground Truth:  [[18.494]]\n",
      "diff:  [[11.01199999]]\n",
      "------------\n",
      "Predict:  [[74.77524989]]\n",
      "Ground Truth:  [[59.1705]]\n",
      "diff:  [[15.60474989]]\n",
      "------------\n",
      "Predict:  [[50.58224997]]\n",
      "Ground Truth:  [[241.722]]\n",
      "diff:  [[191.13975003]]\n",
      "------------\n",
      "Predict:  [[15.73049999]]\n",
      "Ground Truth:  [[39.0855]]\n",
      "diff:  [[23.35500001]]\n",
      "------------\n",
      "Predict:  [[7.6425]]\n",
      "Ground Truth:  [[13.14875]]\n",
      "diff:  [[5.50625]]\n",
      "------------\n",
      "Predict:  [[2.90875]]\n",
      "Ground Truth:  [[12.77833333]]\n",
      "diff:  [[9.86958333]]\n",
      "------------\n",
      "Predict:  [[6.511]]\n",
      "Ground Truth:  [[27.9515]]\n",
      "diff:  [[21.4405]]\n",
      "------------\n",
      "Predict:  [[19.03499996]]\n",
      "Ground Truth:  [[31.53075]]\n",
      "diff:  [[12.49575004]]\n",
      "------------\n",
      "Predict:  [[88.18874971]]\n",
      "Ground Truth:  [[17.894]]\n",
      "diff:  [[70.29474971]]\n",
      "------------\n",
      "Predict:  [[9.3865]]\n",
      "Ground Truth:  [[18.4875]]\n",
      "diff:  [[9.101]]\n",
      "------------\n",
      "Predict:  [[32.56874991]]\n",
      "Ground Truth:  [[13.66325]]\n",
      "diff:  [[18.90549991]]\n",
      "------------\n",
      "Predict:  [[7.93958333]]\n",
      "Ground Truth:  [[7.98341667]]\n",
      "diff:  [[0.04383333]]\n",
      "------------\n",
      "Predict:  [[4.43]]\n",
      "Ground Truth:  [[28.34975]]\n",
      "diff:  [[23.91975]]\n",
      "------------\n",
      "Predict:  [[6.39]]\n",
      "Ground Truth:  [[3.320875]]\n",
      "diff:  [[3.069125]]\n",
      "------------\n",
      "Predict:  [[6.267]]\n",
      "Ground Truth:  [[764.95875]]\n",
      "diff:  [[758.69175]]\n",
      "------------\n",
      "Predict:  [[4.40925]]\n",
      "Ground Truth:  [[6.60758333]]\n",
      "diff:  [[2.19833333]]\n",
      "------------\n",
      "Predict:  [[4.43]]\n",
      "Ground Truth:  [[209.959]]\n",
      "diff:  [[205.529]]\n",
      "------------\n",
      "Predict:  [[59.98225]]\n",
      "Ground Truth:  [[95.23875]]\n",
      "diff:  [[35.2565]]\n",
      "------------\n",
      "Predict:  [[6.511]]\n",
      "Ground Truth:  [[173.597]]\n",
      "diff:  [[167.086]]\n",
      "------------\n",
      "Predict:  [[147.4129999]]\n",
      "Ground Truth:  [[69.2705]]\n",
      "diff:  [[78.1424999]]\n",
      "------------\n",
      "Predict:  [[58.65124995]]\n",
      "Ground Truth:  [[59.0995]]\n",
      "diff:  [[0.44825005]]\n",
      "------------\n",
      "Predict:  [[59.41749995]]\n",
      "Ground Truth:  [[7.9125]]\n",
      "diff:  [[51.50499995]]\n",
      "------------\n",
      "Predict:  [[13.63324997]]\n",
      "Ground Truth:  [[33.87575]]\n",
      "diff:  [[20.24250003]]\n",
      "------------\n",
      "Predict:  [[149.22599991]]\n",
      "Ground Truth:  [[19.6855]]\n",
      "diff:  [[129.54049991]]\n",
      "------------\n",
      "Predict:  [[39.61225]]\n",
      "Ground Truth:  [[121.725]]\n",
      "diff:  [[82.11275]]\n",
      "------------\n",
      "Predict:  [[194.43999987]]\n",
      "Ground Truth:  [[10.47175]]\n",
      "diff:  [[183.96824987]]\n",
      "------------\n",
      "Predict:  [[3.92875]]\n",
      "Ground Truth:  [[12.255]]\n",
      "diff:  [[8.32625]]\n",
      "------------\n",
      "Predict:  [[71.87024994]]\n",
      "Ground Truth:  [[21.2375]]\n",
      "diff:  [[50.63274994]]\n",
      "------------\n",
      "Predict:  [[15.76374998]]\n",
      "Ground Truth:  [[15.146]]\n",
      "diff:  [[0.61774998]]\n",
      "------------\n",
      "Predict:  [[6.55775]]\n",
      "Ground Truth:  [[3.76041667]]\n",
      "diff:  [[2.79733333]]\n",
      "------------\n",
      "Predict:  [[60.05124996]]\n",
      "Ground Truth:  [[11.36375]]\n",
      "diff:  [[48.68749996]]\n",
      "------------\n",
      "Predict:  [[6.39]]\n",
      "Ground Truth:  [[8.7865]]\n",
      "diff:  [[2.3965]]\n",
      "------------\n",
      "Predict:  [[6.38099998]]\n",
      "Ground Truth:  [[177.191]]\n",
      "diff:  [[170.81000002]]\n",
      "------------\n",
      "Predict:  [[3.329]]\n",
      "Ground Truth:  [[33.73683333]]\n",
      "diff:  [[30.40783333]]\n",
      "------------\n",
      "Predict:  [[12.41274998]]\n",
      "Ground Truth:  [[12.20891667]]\n",
      "diff:  [[0.20383332]]\n",
      "------------\n",
      "Predict:  [[13.37724998]]\n",
      "Ground Truth:  [[7.976]]\n",
      "diff:  [[5.40124998]]\n",
      "------------\n",
      "Predict:  [[54.73349995]]\n",
      "Ground Truth:  [[53.462]]\n",
      "diff:  [[1.27149995]]\n",
      "------------\n",
      "Predict:  [[9.3865]]\n",
      "Ground Truth:  [[42.8625]]\n",
      "diff:  [[33.476]]\n",
      "------------\n",
      "Predict:  [[9.69499998]]\n",
      "Ground Truth:  [[4.90075]]\n",
      "diff:  [[4.79424998]]\n",
      "------------\n",
      "Predict:  [[16.79424999]]\n",
      "Ground Truth:  [[25.482]]\n",
      "diff:  [[8.68775001]]\n",
      "------------\n",
      "Predict:  [[5.872]]\n",
      "Ground Truth:  [[5.325]]\n",
      "diff:  [[0.547]]\n",
      "------------\n",
      "Predict:  [[12.52999997]]\n",
      "Ground Truth:  [[4.338]]\n",
      "diff:  [[8.19199997]]\n",
      "------------\n",
      "Predict:  [[3.72049999]]\n",
      "Ground Truth:  [[24.87525]]\n",
      "diff:  [[21.15475001]]\n",
      "------------\n",
      "Predict:  [[3.33475]]\n",
      "Ground Truth:  [[5.20616667]]\n",
      "diff:  [[1.87141667]]\n",
      "------------\n",
      "Predict:  [[7.92399995]]\n",
      "Ground Truth:  [[137.686]]\n",
      "diff:  [[129.76200005]]\n",
      "------------\n",
      "Predict:  [[3.135125]]\n",
      "Ground Truth:  [[3.535375]]\n",
      "diff:  [[0.40025]]\n",
      "------------\n",
      "Predict:  [[57.8385]]\n",
      "Ground Truth:  [[450.4355]]\n",
      "diff:  [[392.597]]\n",
      "------------\n",
      "Predict:  [[76.06474997]]\n",
      "Ground Truth:  [[76.12775]]\n",
      "diff:  [[0.06300003]]\n",
      "------------\n",
      "Predict:  [[42.3465832]]\n",
      "Ground Truth:  [[341.796]]\n",
      "diff:  [[299.4494168]]\n",
      "------------\n",
      "Predict:  [[251.63524978]]\n",
      "Ground Truth:  [[32.9025]]\n",
      "diff:  [[218.73274978]]\n",
      "------------\n",
      "Predict:  [[21.90175]]\n",
      "Ground Truth:  [[21.5485]]\n",
      "diff:  [[0.35325]]\n",
      "------------\n",
      "Predict:  [[2.46]]\n",
      "Ground Truth:  [[7.18625]]\n",
      "diff:  [[4.72625]]\n",
      "------------\n",
      "Predict:  [[4.2570833]]\n",
      "Ground Truth:  [[38.3195]]\n",
      "diff:  [[34.0624167]]\n",
      "------------\n",
      "Predict:  [[59.38549996]]\n",
      "Ground Truth:  [[8.76025]]\n",
      "diff:  [[50.62524996]]\n",
      "------------\n",
      "Predict:  [[144.93374987]]\n",
      "Ground Truth:  [[55.5795]]\n",
      "diff:  [[89.35424987]]\n",
      "------------\n",
      "Predict:  [[232.44424974]]\n",
      "Ground Truth:  [[72.18775]]\n",
      "diff:  [[160.25649974]]\n",
      "------------\n",
      "Predict:  [[4.7885]]\n",
      "Ground Truth:  [[10.833]]\n",
      "diff:  [[6.0445]]\n",
      "------------\n",
      "Predict:  [[107.29174993]]\n",
      "Ground Truth:  [[2.85675]]\n",
      "diff:  [[104.43499993]]\n",
      "------------\n",
      "Predict:  [[195.46699989]]\n",
      "Ground Truth:  [[70.37908333]]\n",
      "diff:  [[125.08791656]]\n",
      "------------\n",
      "Predict:  [[5.76775]]\n",
      "Ground Truth:  [[13.637]]\n",
      "diff:  [[7.86925]]\n",
      "------------\n",
      "Predict:  [[140.64799993]]\n",
      "Ground Truth:  [[188.78475]]\n",
      "diff:  [[48.13675007]]\n",
      "------------\n",
      "Predict:  [[42.34658321]]\n",
      "Ground Truth:  [[40.80725]]\n",
      "diff:  [[1.53933321]]\n",
      "------------\n",
      "Predict:  [[15.11449997]]\n",
      "Ground Truth:  [[12.42883333]]\n",
      "diff:  [[2.68566664]]\n",
      "------------\n",
      "Predict:  [[3.7625]]\n",
      "Ground Truth:  [[9.6185]]\n",
      "diff:  [[5.856]]\n",
      "------------\n",
      "Predict:  [[2.90875]]\n",
      "Ground Truth:  [[34.96225]]\n",
      "diff:  [[32.0535]]\n",
      "------------\n",
      "Predict:  [[18.86741666]]\n",
      "Ground Truth:  [[34.41091667]]\n",
      "diff:  [[15.54350001]]\n",
      "------------\n",
      "Predict:  [[8.04799999]]\n",
      "Ground Truth:  [[6.796]]\n",
      "diff:  [[1.25199999]]\n",
      "------------\n",
      "Predict:  [[32.99349991]]\n",
      "Ground Truth:  [[35.779]]\n",
      "diff:  [[2.78550009]]\n",
      "------------\n",
      "Predict:  [[193.20524977]]\n",
      "Ground Truth:  [[8.87275]]\n",
      "diff:  [[184.33249977]]\n",
      "------------\n",
      "Predict:  [[30.38149999]]\n",
      "Ground Truth:  [[13.47133333]]\n",
      "diff:  [[16.91016666]]\n",
      "------------\n",
      "Predict:  [[24.54025]]\n",
      "Ground Truth:  [[32.754]]\n",
      "diff:  [[8.21375]]\n",
      "------------\n",
      "Predict:  [[18.38574995]]\n",
      "Ground Truth:  [[10.9345]]\n",
      "diff:  [[7.45124995]]\n",
      "------------\n",
      "Predict:  [[14.40674998]]\n",
      "Ground Truth:  [[196.2085]]\n",
      "diff:  [[181.80175002]]\n",
      "------------\n",
      "Predict:  [[12.52999996]]\n",
      "Ground Truth:  [[3.215875]]\n",
      "diff:  [[9.31412496]]\n",
      "------------\n",
      "Predict:  [[132.45675]]\n",
      "Ground Truth:  [[193.8535]]\n",
      "diff:  [[61.39675]]\n",
      "------------\n",
      "Predict:  [[5.87975]]\n",
      "Ground Truth:  [[94.117]]\n",
      "diff:  [[88.23725]]\n",
      "------------\n",
      "Predict:  [[298.97749982]]\n",
      "Ground Truth:  [[164.179]]\n",
      "diff:  [[134.79849982]]\n",
      "------------\n",
      "Predict:  [[188.62799988]]\n",
      "Ground Truth:  [[92.42075]]\n",
      "diff:  [[96.20724988]]\n",
      "------------\n",
      "Predict:  [[51.17874999]]\n",
      "Ground Truth:  [[41.93025]]\n",
      "diff:  [[9.24849999]]\n",
      "------------\n",
      "Predict:  [[4.206]]\n",
      "Ground Truth:  [[5.90333333]]\n",
      "diff:  [[1.69733333]]\n",
      "------------\n",
      "Predict:  [[2.5115]]\n",
      "Ground Truth:  [[26.18166667]]\n",
      "diff:  [[23.67016667]]\n",
      "------------\n",
      "Predict:  [[7.5945]]\n",
      "Ground Truth:  [[7.48325]]\n",
      "diff:  [[0.11125]]\n",
      "------------\n",
      "Predict:  [[68.37649996]]\n",
      "Ground Truth:  [[16.25]]\n",
      "diff:  [[52.12649996]]\n",
      "------------\n",
      "Predict:  [[96.74449994]]\n",
      "Ground Truth:  [[19.63275]]\n",
      "diff:  [[77.11174994]]\n",
      "------------\n",
      "Predict:  [[3.15225]]\n",
      "Ground Truth:  [[4.998]]\n",
      "diff:  [[1.84575]]\n",
      "------------\n",
      "Right Predict:  25\n",
      "--- 0.2732696533203125 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Step6 predict for all workload B\n",
    "right_count = 0\n",
    "for i in range(100):\n",
    "    tmp_target_config = workload_B_target[i][knobs_latency_list].values\n",
    "    #print(tmp_target_config)\n",
    "    tmp_target_config_norm = inputScalar_list[i].transform(tmp_target_config)\n",
    "    #print(tmp_target_config_norm)\n",
    "    tmp_max = (predictScalar_list[i].data_max_[0])\n",
    "    tmp_min = (predictScalar_list[i].data_min_[0])\n",
    "    tmp_predict = predictGPRmodel_list[i].predict(tmp_target_config_norm)\n",
    "    tmp_predict_re = tmp_predict * (tmp_max - tmp_min) + tmp_min\n",
    "    tmp_diff = np.abs(tmp_predict_re - workload_B_target[i][latency].values)\n",
    "    print('Predict: ',tmp_predict_re)\n",
    "    print('Ground Truth: ',workload_B_target[i][latency].values)\n",
    "    print('diff: ',tmp_diff)\n",
    "    if tmp_diff < 3:\n",
    "        right_count += 1\n",
    "    print('------------')\n",
    "    \n",
    "print('Right Predict: ', right_count)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
